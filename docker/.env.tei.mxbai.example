# TEI standalone environment example (Mixedbread, broader hardware profile)
# Copy to .env.tei.mxbai and run with:
# docker compose --env-file .env.tei.mxbai -f docker-compose.tei.mxbai.yaml up -d

# TEI image:
# - CPU default works for most users
# - For NVIDIA GPUs (RTX 40xx), set:
#   TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:89-1.8.1
TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:cpu-1.8.1

# Service endpoint (host high-port -> container 80)
TEI_HTTP_PORT=53021

# Model (1024-dim embeddings)
TEI_EMBEDDING_MODEL=mixedbread-ai/mxbai-embed-large-v1
TEI_DTYPE=float32
TEI_POOLING=cls
TEI_DEFAULT_PROMPT=Represent this sentence for searching relevant passages:

# Conservative defaults for broad compatibility
TEI_MAX_CONCURRENT_REQUESTS=16
TEI_MAX_BATCH_TOKENS=8192
TEI_MAX_BATCH_REQUESTS=16
TEI_MAX_CLIENT_BATCH_SIZE=16
TEI_TOKENIZATION_WORKERS=4

# Storage
TEI_DATA_DIR=/home/jmagar/appdata/tei-mxbai

# Runtime
OMP_NUM_THREADS=4
MKL_NUM_THREADS=4
TOKENIZERS_PARALLELISM=true
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
RUST_LOG=text_embeddings_router=info
HF_HUB_ENABLE_HF_TRANSFER=1

# Required for gated/private models. Optional for public models.
HF_TOKEN=
