# TEI environment template: NVIDIA RTX 3050 (8GB VRAM)
# Copy to repo root as .env.tei.mxbai, then run:
# docker compose --env-file .env.tei.mxbai -f docker/docker-compose.tei.mxbai.yaml up -d

# For NVIDIA GPUs (instead of CPU image), set:
TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:89-1.8.1

# Service endpoint (host high-port -> container 80)
TEI_HTTP_PORT=53021

# Model + runtime tuned for 8GB VRAM
TEI_EMBEDDING_MODEL=mixedbread-ai/mxbai-embed-large-v1
TEI_DTYPE=float16
TEI_POOLING=cls
TEI_DEFAULT_PROMPT=Represent this sentence for searching relevant passages:

# Conservative throughput for RTX 3050
TEI_MAX_CONCURRENT_REQUESTS=8
TEI_MAX_BATCH_TOKENS=6144
TEI_MAX_BATCH_REQUESTS=8
TEI_MAX_CLIENT_BATCH_SIZE=8
TEI_TOKENIZATION_WORKERS=4

# Storage
TEI_DATA_DIR=./data/tei-mxbai

# GPU selection
NVIDIA_VISIBLE_DEVICES=0
CUDA_VISIBLE_DEVICES=0

# Runtime
OMP_NUM_THREADS=4
MKL_NUM_THREADS=4
TOKENIZERS_PARALLELISM=true
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
RUST_LOG=text_embeddings_router=info
HF_HUB_ENABLE_HF_TRANSFER=1

# Required for gated/private models. Optional for public models.
HF_TOKEN=
