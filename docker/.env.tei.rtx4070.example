# TEI environment template: NVIDIA RTX 4070 (12GB VRAM)
# Copy to repo root as .env.tei, then run:
# docker compose --env-file .env.tei -f docker/docker-compose.tei.yaml up -d

# Service endpoint (host high-port -> container 80)
TEI_HTTP_PORT=53020

# Model + runtime
TEI_EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
TEI_DTYPE=float16
TEI_POOLING=last-token
TEI_DEFAULT_PROMPT=Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery:

# Throughput tuning for RTX 4070
TEI_MAX_CONCURRENT_REQUESTS=80
TEI_MAX_BATCH_TOKENS=163840
TEI_MAX_BATCH_REQUESTS=80
TEI_MAX_CLIENT_BATCH_SIZE=128
TEI_TOKENIZATION_WORKERS=8

# Storage
TEI_DATA_DIR=./data/tei

# GPU selection
NVIDIA_VISIBLE_DEVICES=0
CUDA_VISIBLE_DEVICES=0

# Runtime
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
OMP_NUM_THREADS=8
MKL_NUM_THREADS=8
TOKENIZERS_PARALLELISM=true
CUDA_CACHE_DISABLE=0
RUST_LOG=text_embeddings_router=info
HF_HUB_ENABLE_HF_TRANSFER=1

# Required for gated/private models. Optional for public models.
HF_TOKEN=
